{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Create a Semantic Segmentation Network\r\n",
        "\r\n",
        "Copyright (c) Microsoft Corporation.\r\n",
        "Licensed under the MIT license.\r\n",
        "\r\n",
        "This tutorial will walk you through a typical workflow for creating a new model for the Azure Percept DK.\r\n",
        "See the [GitHub](https://github.com/microsoft/azure-percept-advanced-development) for the rest of the\r\n",
        "steps to port this model to the device."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell imports everything we need\r\n",
        "from azureml.core import ComputeTarget\r\n",
        "from azureml.core import Workspace\r\n",
        "from azureml.core import Dataset\r\n",
        "from azureml.core.compute import AmlCompute\r\n",
        "from azureml.core.compute_target import ComputeTargetException\r\n",
        "from azureml.core import ScriptRunConfig\r\n",
        "from azureml.core import Environment\r\n",
        "from azureml.core import Experiment\r\n",
        "from azureml.tensorboard import Tensorboard\r\n",
        "from PIL import Image\r\n",
        "from torchvision import transforms as T\r\n",
        "from tqdm import tqdm\r\n",
        "import cv2\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import shutil\r\n",
        "import torch\r\n",
        "import train  # Must be in the directory that contains the train.py script!\r\n",
        "\r\n",
        "%matplotlib inline\r\n",
        "plt.rcParams[\"figure.figsize\"] = (20, 20)\r\n",
        "\r\n",
        "%pip install onnxruntime\r\n",
        "import onnx\r\n",
        "import onnxruntime"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618418047745
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We'll put everything here\r\n",
        "topdir = !pwd\r\n",
        "topdir = topdir[0]\r\n",
        "topdir"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618418092084
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\r\n",
        "\r\n",
        "First, we'll need a dataset.\r\n",
        "\r\n",
        "We'll use the Pascal VOC dataset (http://host.robots.ox.ac.uk/pascal/VOC/).\r\n",
        "The VOC dataset does not have a whole lot of images, so let's combine the train and val split (which are initially\r\n",
        "split at about 50/50) and resplit at 90/10.\r\n",
        "\r\n",
        "Also, the Pascal VOC dataset has 20 classes - let's combine them all into just 5:\r\n",
        "\r\n",
        "* Background\r\n",
        "* Person\r\n",
        "* Animal\r\n",
        "* Vehicle\r\n",
        "* Indoor"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Warning! The first time you run this cell will take around an hour or two, as the whole dataset must be downloaded to your workspace and then extracted.\r\n",
        "# Subsequent times will just reuse the cached dataset (unless you delete it).\r\n",
        "imgsize = 128  # We will resize to this value\r\n",
        "dataset_dir = os.path.join(topdir, \"dataset\")\r\n",
        "download = not os.path.isdir(dataset_dir)\r\n",
        "x_transforms, y_transforms = train.get_transforms(size=imgsize)\r\n",
        "dataset_train = train.TransformedVocDataset(dataset_dir, image_set=\"train\", download=download, x_transforms=x_transforms, y_transforms=y_transforms)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618418094890
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell will also take about an hour or two. Frown.\r\n",
        "dataset_dir_val = os.path.join(topdir, \"dataset-val\")\r\n",
        "download = not os.path.isdir(dataset_dir_val)\r\n",
        "dataset_val = train.TransformedVocDataset(dataset_dir_val, image_set=\"val\", download=download, x_transforms=x_transforms, y_transforms=y_transforms)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618418099217
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Since VOC is a pretty small dataset, but they have a 50/50 split on train and val,\r\n",
        "# let's combine their train and val splits and resplit at like 90/10, which will\r\n",
        "# give us some more data to work with. (The resplitting occurs later on - here we are just concating the datasets).\r\n",
        "dataset = torch.utils.data.ConcatDataset([dataset_train, dataset_val])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618418102093
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure the dataset works and take a look at a few images\r\n",
        "# to see if we like the augmentations.\r\n",
        "nimgs = 5\r\n",
        "for i in range(0, nimgs * 2, 2):\r\n",
        "    x, y = dataset[i]\r\n",
        "\r\n",
        "    # Convert X to an image\r\n",
        "    x = T.ToPILImage()(x)\r\n",
        "\r\n",
        "    # Convert Y to an image\r\n",
        "    y = dataset_train.mask_tensor_to_pil_image(y)\r\n",
        "\r\n",
        "    plt.subplot(nimgs, 2, i + 1)\r\n",
        "    plt.imshow(x)\r\n",
        "\r\n",
        "    plt.subplot(nimgs, 2, i + 2)\r\n",
        "    plt.imshow(y)\r\n",
        "\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618418107893
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# How many images?\r\n",
        "print(len(dataset))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618418115726
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine how many of each pixel are of each class so we can weight the loss function appropriately\r\n",
        "histogram = [0 for _ in range(len(train.VOC_CLASSES_COMBINED))]\r\n",
        "\r\n",
        "for _, y in tqdm(dataset):\r\n",
        "    for classidx in range(len(train.VOC_CLASSES_COMBINED)):\r\n",
        "        histogram[classidx] += torch.sum(y == classidx).item()\r\n",
        "\r\n",
        "weights = [min(histogram) / histogram[i] for i in range(len(train.VOC_CLASSES_COMBINED))]\r\n",
        "\r\n",
        "for i, count in enumerate(histogram):\r\n",
        "    print(f\"{train.VOC_CLASSES_COMBINED[i]}: {count}, or about {(100.0 * count / sum(histogram)):.2f}% of the dataset. So weighting with {weights[i]}\")\r\n",
        "\r\n",
        "# Now adjust the background weight: I find that we need to penalize false positives a bit more heavily\r\n",
        "weights[0] *= 10\r\n",
        "print(f\"Background weight updated to {weights[0]}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618337159515
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Network\r\n",
        "\r\n",
        "Here we create an AML experiment and submit it to the created GPU node. You can track the progress using AML logging.\r\n",
        "\r\n",
        "Of course, you don't have to use Azure Machine Learning services. You can use whatever workflow you are used to. This notebook\r\n",
        "just walks you through an example that uses AML, but under the hood, it is just running PyTorch.\r\n",
        "\r\n",
        "As long as you end up with an ONNX model at the end (or a Tensorflow model), you should be good."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the default datastore associated with this workspace\r\n",
        "ws = Workspace.from_config()\r\n",
        "datastore = ws.get_default_datastore()\r\n",
        "datastore_data_path = \"datasets/voc-segmentation-train-tutorial\"\r\n",
        "datastore_data_path_val = \"datasets/voc-segmentation-val-tutorial\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618418127710
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the dataset to the datastore (this cell takes like 10 minutes but only ever needs to be run once)\r\n",
        "datastore.upload(src_dir=dataset_dir, target_path=datastore_data_path)\r\n",
        "datastore.upload(src_dir=dataset_dir_val, target_path=datastore_data_path_val)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618338133805
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Register the uploaded dataset as an AML Dataset object\r\n",
        "amldataset = Dataset.File.from_files(path=(datastore, datastore_data_path))\r\n",
        "amldataset = amldataset.register(workspace=ws, name=\"voc-segmentation-train-tutorial\", description=\"VOC for Segmentation\")\r\n",
        "amldataset_val = Dataset.File.from_files(path=(datastore, datastore_data_path_val))\r\n",
        "amldataset_val = amldataset_val.register(workspace=ws, name=\"voc-segmentation-val-tutorial\", description=\"VOC (val) for Segmentation\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618338142278
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Provision a compute cluster if you don't already have one\r\n",
        "cluster_name = \"gpu1\"\r\n",
        "try:\r\n",
        "    cluster = ComputeTarget(workspace=ws, name=cluster_name)\r\n",
        "    print(\"Found an existing cluster. We will use this one.\")\r\n",
        "except ComputeTargetException:\r\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size=\"STANDARD_NC6\",  # Make sure to choose one that your subscription has\r\n",
        "                                                           idle_seconds_before_scaledown=2400,\r\n",
        "                                                           min_nodes=0,\r\n",
        "                                                           max_nodes=1)\r\n",
        "    cluster = ComputeTarget.create(ws, cluster_name, compute_config)\r\n",
        "cluster.wait_for_completion(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618338847960
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn the class weights into a string for ingestion by the training script\r\n",
        "class_weights_string = \"\"\r\n",
        "for w in weights:\r\n",
        "    class_weights_string += f\"{w} \"\r\n",
        "class_weights_string = class_weights_string.rstrip(\" \")\r\n",
        "class_weights_string"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618338851332
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean up previous runs before submitting\r\n",
        "shutil.rmtree(\"logs\", ignore_errors=True)\r\n",
        "shutil.rmtree(\"outputs\", ignore_errors=True)\r\n",
        "!mkdir -p logs\r\n",
        "\r\n",
        "# Create the experiment and submit it. This is the cell that actually kicks off training.\r\n",
        "experiment = Experiment(workspace=ws, name=\"AzurePerceptDKTutorial\")\r\n",
        "config = ScriptRunConfig(\r\n",
        "    source_directory=\".\",\r\n",
        "    script=\"train.py\",\r\n",
        "    compute_target=cluster_name,\r\n",
        "    arguments=[\r\n",
        "        \"--dataset\", amldataset.as_named_input(\"input_train\").as_mount(),\r\n",
        "        \"--dataset-val\", amldataset_val.as_named_input(\"input_val\").as_mount(),\r\n",
        "        \"--resize\", imgsize,\r\n",
        "        \"--batchsize\", 128,\r\n",
        "        \"--learning-rate\", 0.001,\r\n",
        "        \"--nepochs\", 400,\r\n",
        "        \"--split\", 0.9,\r\n",
        "        \"--weights\", class_weights_string\r\n",
        "    ]\r\n",
        ")\r\n",
        "\r\n",
        "# Set up the training environment (see https://docs.microsoft.com/en-us/azure/machine-learning/resource-curated-environments\r\n",
        "# for a list of curated environments if you don't want to create one from a requirements.txt or a Conda YAML)\r\n",
        "env = Environment.from_pip_requirements(name=\"PyTorch-AzurePerceptDK-Env\", file_path=\"requirements.txt\")\r\n",
        "env.docker.enabled = True\r\n",
        "env.docker.base_image = \"mcr.microsoft.com/azureml/openmpi3.1.2-cuda10.2-cudnn7-ubuntu18.04\"\r\n",
        "config.run_config.environment = env\r\n",
        "\r\n",
        "# Kick off the experiment!\r\n",
        "run = experiment.submit(config)\r\n",
        "\r\n",
        "# Print out a link to the experiment for tracking using AML\r\n",
        "print(\"Submitted to compute cluster. Click the link below.\")\r\n",
        "print(run.get_portal_url())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618340061393
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tensorboard\r\n",
        "tb = Tensorboard([run])\r\n",
        "tb.start()\r\n",
        "print(\"Click the link above to view the output on TensorBoard\")\r\n",
        "\r\n",
        "# Block until run completes.\r\n",
        "run.wait_for_completion(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618283604157
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tb.stop()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618417984895
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the results of all our efforts\r\n",
        "experiment = Experiment(workspace=ws, name=\"AzurePerceptDKTutorial\")\r\n",
        "runs = experiment.get_runs()\r\n",
        "\r\n",
        "# Grab the run we used. It is likely the user has stopped and come back after the run,\r\n",
        "# so if that's the case, let's look through the runs under the experiment name to find\r\n",
        "# the latest completed one (get_runs() is in reverse chronological order).\r\n",
        "try:\r\n",
        "    completed_run = run\r\n",
        "except NameError:\r\n",
        "    completed_run = None\r\n",
        "    for r in runs:\r\n",
        "        if r.get_status() == \"Completed\":\r\n",
        "            completed_run = r\r\n",
        "            break\r\n",
        "\r\n",
        "if completed_run is None:\r\n",
        "    print(\"No runs completed yet.\")\r\n",
        "else:\r\n",
        "    print(\"Downloading outputs...\")\r\n",
        "    completed_run.download_files(\"outputs\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618417989920
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try out our Model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load up the model\r\n",
        "model = train.UNet(len(train.VOC_CLASSES_COMBINED))\r\n",
        "model.load_state_dict(torch.load(\"outputs/model.pth\"))\r\n",
        "model.eval()  # Don't forget to set the model to eval mode!"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618418153132
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run some examples through it\r\n",
        "# NOTE! These images may have been in the training split, so don't get too excited :P\r\n",
        "# We don't know exactly which images went into the training split\r\n",
        "# (we could figure it out, but the point here is just to make sure that we can\r\n",
        "# load the model and then later convert it to ONNX and still get the same results,\r\n",
        "# NOT to show that the model does a good job).\r\n",
        "nimgs = 5\r\n",
        "for i in range(0, nimgs * 3, 3):\r\n",
        "    x, y = dataset[i]\r\n",
        "\r\n",
        "    # Convert X to an image\r\n",
        "    ximg = T.ToPILImage()(x)\r\n",
        "\r\n",
        "    # Convert Y to an image\r\n",
        "    yimg = dataset_train.mask_tensor_to_pil_image(y)\r\n",
        "\r\n",
        "    # Run X through the model and convert output to an image\r\n",
        "    pred = model(x.unsqueeze(0))\r\n",
        "    predimg = train.TransformedVocDataset.one_hot_tensor_to_pil_image(pred[0])\r\n",
        "\r\n",
        "    plt.subplot(nimgs, 3, i + 1)\r\n",
        "    plt.imshow(ximg)\r\n",
        "\r\n",
        "    plt.subplot(nimgs, 3, i + 2)\r\n",
        "    plt.imshow(yimg)\r\n",
        "\r\n",
        "    plt.subplot(nimgs, 3, i + 3)\r\n",
        "    plt.imshow(predimg)\r\n",
        "\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618418164765
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's run it on a video from the OpenVINO model zoo test website. We can directly\r\n",
        "# compare this model in PyTorch to the same model in OpenVINO later using this video.\r\n",
        "!wget https://github.com/intel-iot-devkit/sample-videos/raw/master/person-bicycle-car-detection.mp4 -O outputs/movie.mp4"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618418179952
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cap = cv2.VideoCapture(os.path.join(\"outputs\", \"movie.mp4\"))\r\n",
        "if not cap.isOpened():\r\n",
        "    print(\"Could not open video.\")\r\n",
        "\r\n",
        "# Spool out all the frames from the video\r\n",
        "frames = []\r\n",
        "while cap.isOpened():\r\n",
        "    ret, frame = cap.read()\r\n",
        "    if ret:\r\n",
        "        # Change from OpenCV's BGR to RGB\r\n",
        "        frame = frame[:, :, ::-1]\r\n",
        "        frames.append(frame)\r\n",
        "    else:\r\n",
        "        break\r\n",
        "cap.release()\r\n",
        "\r\n",
        "# Choose some interesting frames to display\r\n",
        "imgs = [\r\n",
        "    Image.fromarray(frames[40]),\r\n",
        "    Image.fromarray(frames[50]),\r\n",
        "    Image.fromarray(frames[200]),\r\n",
        "    Image.fromarray(frames[325]),\r\n",
        "    Image.fromarray(frames[560]),\r\n",
        "]\r\n",
        "\r\n",
        "shutil.rmtree(os.path.join(\"outputs\", \"input-imgs\"), ignore_errors=True)\r\n",
        "os.makedirs(os.path.join(\"outputs\", \"input-imgs\"))\r\n",
        "\r\n",
        "nimgs = len(imgs)\r\n",
        "j = 0\r\n",
        "for i in range(0, nimgs * 2, 2):\r\n",
        "    ximg = imgs[j]\r\n",
        "    j += 1\r\n",
        "\r\n",
        "    # Preprocess X\r\n",
        "    ximg = T.Resize((imgsize, imgsize))(ximg)\r\n",
        "    x = T.ToTensor()(ximg)  # Converts from [0, 255] -> [0.0, 1.0]\r\n",
        "\r\n",
        "    # Save ximg for later\r\n",
        "    ximg.save(os.path.join(\"outputs\", \"input-imgs\", f\"img{j}.png\"))\r\n",
        "    \r\n",
        "    # Run X through the model and convert output to an image\r\n",
        "    pred = model(x.unsqueeze(0))\r\n",
        "    predimg = train.TransformedVocDataset.one_hot_tensor_to_pil_image(pred[0])\r\n",
        "\r\n",
        "    plt.subplot(nimgs, 2, i + 1)\r\n",
        "    plt.imshow(ximg)\r\n",
        "\r\n",
        "    plt.subplot(nimgs, 2, i + 2)\r\n",
        "    plt.imshow(predimg)\r\n",
        "\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618418189733
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert to ONNX\r\n",
        "\r\n",
        "We need to convert the model to ONNX runtime, since ultimately we need it in OpenVINO IR or .blob format, and OpenVINO does not understand Pytorch's\r\n",
        "native model format."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to ONNX\r\n",
        "dummy_input, _ = dataset[0]\r\n",
        "dummy_input = dummy_input.unsqueeze(0)  # Add a batch dimension\r\n",
        "torch.onnx.export(model, dummy_input, \"outputs/model.onnx\", export_params=True, input_names=[\"input\"], output_names=[\"output\"], verbose=False)\r\n",
        "\r\n",
        "# Load it back into memory to make sure that's possible\r\n",
        "onnx_model = onnx.load(\"outputs/model.onnx\")\r\n",
        "onnx.checker.check_model(onnx_model)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618418204272
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure it still works\r\n",
        "ort_session = onnxruntime.InferenceSession(\"outputs/model.onnx\")\r\n",
        "\r\n",
        "nimgs = 5\r\n",
        "for i in range(0, nimgs * 3, 3):\r\n",
        "    x, y = dataset[i]\r\n",
        "\r\n",
        "    # Convert X to an image\r\n",
        "    ximg = T.ToPILImage()(x)\r\n",
        "\r\n",
        "    # Convert Y to an image\r\n",
        "    yimg = dataset_train.mask_tensor_to_pil_image(y)\r\n",
        "\r\n",
        "    # Run X through the model and convert output to an image\r\n",
        "    ort_inputs = {ort_session.get_inputs()[0].name: x.unsqueeze(0).detach().cpu().numpy()}\r\n",
        "    pred = ort_session.run(None, ort_inputs)\r\n",
        "    pred = pred[0].squeeze()  # ORT returns a list of outputs, but we only have one output.\r\n",
        "    predimg = train.TransformedVocDataset.one_hot_tensor_to_pil_image(torch.tensor(pred))\r\n",
        "\r\n",
        "    plt.subplot(nimgs, 3, i + 1)\r\n",
        "    plt.imshow(ximg)\r\n",
        "\r\n",
        "    plt.subplot(nimgs, 3, i + 2)\r\n",
        "    plt.imshow(yimg)\r\n",
        "\r\n",
        "    plt.subplot(nimgs, 3, i + 3)\r\n",
        "    plt.imshow(predimg)\r\n",
        "\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618418213023
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Convert the ONNX Model to OpenVINO IR and then to Blob Format\r\n",
        "\r\n",
        "The Azure Percept DK requires the device to be in OpenVINO IR or OpenVINO Myriad X blob format.\r\n",
        "So let's convert it."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use openvino/ubuntu18_dev:2021.1 Docker image to do this\r\n",
        "# See the documentation: https://docs.openvinotoolkit.org/latest/openvino_docs_MO_DG_prepare_model_convert_model_Converting_Model.html\r\n",
        "# Note that we need to scale input values by 255, since our network was trained with normalized\r\n",
        "# inputs, but our inputs are going to be uint8 values from the camera.\r\n",
        "# This means that the OpenVINO network we create here will expect inputs in [0, 255] NOT [0.0, 1.0].\r\n",
        "!docker run --rm -v `realpath outputs`:/blah -w /blah openvino/ubuntu18_dev:2021.1 \\\r\n",
        "    python3 \"/opt/intel/openvino_2021/deployment_tools/model_optimizer/mo.py\" \\\r\n",
        "    --input_model \"./model.onnx\" -o \".\" --input \"input\" --output \"output\" --scale 255"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618418248112
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the same Docker image for model conversion from IR to blob\r\n",
        "!docker run --rm -v `realpath outputs`:/blah -w /blah openvino/ubuntu18_dev:2021.1 \\\r\n",
        "    /bin/bash -c \"source /opt/intel/openvino/bin/setupvars.sh && /opt/intel/openvino_2021/deployment_tools/inference_engine/lib/intel64/myriad_compile \\\r\n",
        "    -m ./model.xml \\\r\n",
        "    -o ./model.blob \\\r\n",
        "    -VPU_NUMBER_OF_SHAVES 8 \\\r\n",
        "    -VPU_NUMBER_OF_CMX_SLICES 8 \\\r\n",
        "    -ip U8 \\\r\n",
        "    -op FP32\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618418278646
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify that the OpenVINO IR Model Works as Expected\r\n",
        "\r\n",
        "Let's sanity check our parameters that we used when converting to OpenVINO by\r\n",
        "using the OpenVINO Inference Engine. Its outputs should be very close to the PyTorch outputs\r\n",
        "from earlier."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile \"outputs/validate_using_openvino.py\"\r\n",
        "from openvino.inference_engine import IECore\r\n",
        "from PIL import Image\r\n",
        "import numpy as np\r\n",
        "import os\r\n",
        "import shutil\r\n",
        "ie = IECore()\r\n",
        "\r\n",
        "imgsize = int(os.environ['IMGSIZE'])\r\n",
        "\r\n",
        "net = ie.read_network(\"model.xml\")\r\n",
        "batch, channels, height, width = net.input_info[\"input\"].input_data.shape\r\n",
        "exec_net = ie.load_network(network=net, device_name=\"CPU\")\r\n",
        "\r\n",
        "# Read in the images we are going to be using\r\n",
        "# They've already been preprocessed in a previous cell\r\n",
        "imgs = [Image.open(os.path.join(\"input-imgs\", fname)) for fname in os.listdir(\"input-imgs\") if fname.endswith(\".png\")]\r\n",
        "\r\n",
        "shutil.rmtree(\"output-imgs\", ignore_errors=True)\r\n",
        "os.makedirs(\"output-imgs\")\r\n",
        "\r\n",
        "# Run them through the network\r\n",
        "for i, img in enumerate(imgs):\r\n",
        "    # Convert to numpy, add batch dimension, and permute\r\n",
        "    img = np.transpose(img, (2, 0, 1))\r\n",
        "    img = np.expand_dims(img, 0)\r\n",
        "    res = exec_net.infer(inputs={\"input\": img})\r\n",
        "    res = res[\"output\"]\r\n",
        "    # Output shape is (1, 5, imgsize, imgsize)\r\n",
        "    # Remove batch dimension\r\n",
        "    res = res.reshape((5, imgsize, imgsize))\r\n",
        "    # Save results as files (we'll look at them outside the Docker container)\r\n",
        "    np.save(os.path.join(\"output-imgs\", f\"output-img{i}\"), res)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1617659845951
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Launch Docker\r\n",
        "!docker run --rm -e IMGSIZE=$imgsize -v `realpath outputs`:/blah -w /blah openvino/ubuntu18_runtime:2021.1 \\\r\n",
        "    /bin/bash -c \"python3 -m pip install openvino && \\\r\n",
        "                  python3 -m pip install Pillow && \\\r\n",
        "                  python3 validate_using_openvino.py\""
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618419861270
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display using the images we created with OpenVINO\r\n",
        "imgfpaths = [os.path.join(\"outputs\", \"output-imgs\", fname) for fname in os.listdir(os.path.join(\"outputs\", \"output-imgs\"))]\r\n",
        "imgs = [np.load(fpath) for fpath in imgfpaths]\r\n",
        "imgs = [train.TransformedVocDataset.one_hot_tensor_to_pil_image(torch.tensor(arr)) for arr in imgs]\r\n",
        "\r\n",
        "inputfpaths = [os.path.join(\"outputs\", \"input-imgs\", fname) for fname in os.listdir(os.path.join(\"outputs\", \"input-imgs\"))]\r\n",
        "inputs = [Image.open(i) for i in inputfpaths]\r\n",
        "\r\n",
        "nimgs = len(imgs)\r\n",
        "assert nimgs == len(inputs), f\"Length of input images ({len(inputs)}) != length of output images ({len(imgs)})\"\r\n",
        "\r\n",
        "j = 0\r\n",
        "for i in range(0, nimgs * 2, 2):\r\n",
        "    ximg = inputs[j]\r\n",
        "    predimg = imgs[j]\r\n",
        "    j += 1\r\n",
        "\r\n",
        "    plt.subplot(nimgs, 2, i + 1)\r\n",
        "    plt.imshow(ximg)\r\n",
        "\r\n",
        "    plt.subplot(nimgs, 2, i + 2)\r\n",
        "    plt.imshow(predimg)\r\n",
        "\r\n",
        "plt.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618419874550
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Package the Model Up for Deployment\r\n",
        "\r\n",
        "In this section, we upload the converted model to an Azure blob storage and then manipulate\r\n",
        "your device's module twin's \"ModelZipUrl\" property to point to it.\r\n",
        "\r\n",
        "Note that this won't work until you complete the rest of the tutorial in the GitHub! You haven't\r\n",
        "implemented a G-API graph for this model yet, so it won't run on the device.\r\n",
        "\r\n",
        "So go do that, and then come back when it tells you."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's create a zip file that contains the following items:\r\n",
        "\r\n",
        "1. Our labels.txt file\r\n",
        "1. Our model.blob file (we could do the IR files instead, but it is better to use the .blob file if you have one)\r\n",
        "1. A config.json file that will tell the azureeyemodule application where to find the files and what parser to use."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-import stuff if we have sensibly turned off this node while doing the rest of the tutorial\r\n",
        "from azureml.core import Workspace\r\n",
        "from azure.iot.hub import IoTHubRegistryManager\r\n",
        "from azure.iot.hub.models import Twin, TwinProperties\r\n",
        "import sys\r\n",
        "%pip install azure-storage-blob==2.1.0 msrest\r\n",
        "%pip install azure-iot-hub"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618419895040
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 'outputs/config.json'\r\n",
        "{\r\n",
        "    \"DomainType\": \"unet-seg\",\r\n",
        "    \"ModelFileName\": \"model.blob\",\r\n",
        "    \"LabelFileName\": \"labels.txt\"\r\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile 'outputs/labels.txt'\r\n",
        "background\r\n",
        "person\r\n",
        "animal\r\n",
        "vehicle\r\n",
        "indoor"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the pwd again\r\n",
        "pwd = !pwd\r\n",
        "pwd = pwd[0]\r\n",
        "pwd"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618419900344
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd outputs && zip model.zip model.blob config.json labels.txt && cd $pwd"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1617826076267
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's upload this zip archive to storage."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the default datatstore for upload\r\n",
        "ws = Workspace.from_config()\r\n",
        "ds = ws.get_default_datastore()\r\n",
        "print(ds.name, ds.datastore_type, ds.account_name, ds.container_name)\r\n",
        "\r\n",
        "ds.upload_files(['outputs/model.zip'], target_path='tutorial-models', overwrite=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618419913555
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate download SAS URL for model.zip\r\n",
        "from datetime import datetime, timedelta\r\n",
        "from azure.storage.blob import (\r\n",
        "    BlockBlobService,\r\n",
        "    ContainerPermissions,\r\n",
        "    BlobPermissions,\r\n",
        "    PublicAccess,\r\n",
        ")\r\n",
        "   \r\n",
        "AZURE_ACC_NAME = ds.account_name\r\n",
        "AZURE_PRIMARY_KEY = ds.account_key\r\n",
        "AZURE_CONTAINER = ds.container_name\r\n",
        "AZURE_BLOB=ds.name\r\n",
        "AZURE_File='tutorial-models/model.zip' \r\n",
        "\r\n",
        "block_blob_service = BlockBlobService(account_name=AZURE_ACC_NAME, account_key=AZURE_PRIMARY_KEY)\r\n",
        "\r\n",
        "# We'll expire this SAS in 30 days.\r\n",
        "sas_url = block_blob_service.generate_blob_shared_access_signature(AZURE_CONTAINER,\r\n",
        "                                                                   AZURE_File,\r\n",
        "                                                                   permission=BlobPermissions.READ,\r\n",
        "                                                                   expiry= datetime.utcnow() + timedelta(hours=30*24))\r\n",
        "downloadurl ='https://'+AZURE_ACC_NAME+'.blob.core.windows.net/'+AZURE_CONTAINER+'/'+AZURE_File+'?'+sas_url\r\n",
        "print(downloadurl)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1618419916155
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you copy the link above ^^^^^^^^^, and put it in your ModelZipUrl right now, you should be able to run this model.\r\n",
        "# But let's go ahead and do it from here, just for the heck of it.\r\n",
        "\r\n",
        "# Incorporate the connection string, device_id and the module_id values from your IoT Hub\r\n",
        "# Go to https://portal.azure.com\r\n",
        "# Select your IoT Hub\r\n",
        "# Click on Shared access policies\r\n",
        "# Click 'service' policy on the right (or another policy having 'service connect' permission)\r\n",
        "# Copy Connection string--primary key\r\n",
        "\r\n",
        "CONNECTION_STRING = \"<YOUR-CONNECTION-STRING-PRIMARY-KEY>\"\r\n",
        "\r\n",
        "DEVICE_ID = \"<YOUR-DEVICE-NAME>\"\r\n",
        "# If you have changed the name of the azureeyemodule for some reason,\r\n",
        "# you will need to change it here too.\r\n",
        "MODULE_ID = \"azureeyemodule\"\r\n",
        "\r\n",
        "iothub_registry_manager = IoTHubRegistryManager(CONNECTION_STRING)\r\n",
        "module_twin = iothub_registry_manager.get_module_twin(DEVICE_ID, MODULE_ID)\r\n",
        "\r\n",
        "print ( \"\" )\r\n",
        "print ( \"Module twin properties before update    :\" )\r\n",
        "print ( \"{0}\".format(module_twin.properties) )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Update twin\r\n",
        "twin_patch = Twin()\r\n",
        "twin_patch.properties = TwinProperties(desired={\"ModelZipUrl\": downloadurl})\r\n",
        "updated_module_twin = iothub_registry_manager.update_module_twin(DEVICE_ID, MODULE_ID, twin_patch, module_twin.etag)\r\n",
        "\r\n",
        "print ( \"\" )\r\n",
        "print ( \"Module twin properties after update     :\" )\r\n",
        "print ( \"{0}\".format(updated_module_twin.properties) )"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3-azureml",
      "language": "python",
      "display_name": "Python 3.6 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python3-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}