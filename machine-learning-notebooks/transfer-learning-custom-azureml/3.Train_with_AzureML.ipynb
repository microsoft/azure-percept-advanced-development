{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c0595d5",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation.<br>\n",
    "Licensed under the MIT License.\n",
    "\n",
    "# 3. Train a Model in the Cloud with the Azure ML Python SDK\n",
    "\n",
    "In this notebook we will:\n",
    "- Use Azure ML Python SDK as a tool to train a TensorFlow object detection model on your own data\n",
    "\n",
    "This notebook trains an SSD-MobileNet V2 model using the TensorFlow (v1) Object Detection API.  Here, we have created a custom dockerfile that sets up the Object Detection API library and dependencies for training the model in the cloud.\n",
    "\n",
    "Note:  the first time the experiment is run with Azure ML, it could take up to 30 min to finish as it will need to build the TF Object Detection API (one-time process).  Subsequent runs will be faster.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "- Azure ML Workspace - [Create in Azure Portal](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-manage-workspace?tabs=azure-portal)\n",
    "- Data uploaded to the default DataStore\n",
    "- COCO formatted labels file from the Azure ML Labeling Project, downloaded to this folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2309e4aa",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f13f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core import Dataset\n",
    "from azureml.core import Environment\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core import ScriptRunConfig\n",
    "from azureml.core import Experiment, Run\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from uuid import uuid4\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from azureml.core import VERSION\n",
    "print(VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a4e5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User defined (CHANGE AS NEEDED!)\n",
    "num_classes = 3\n",
    "num_epochs = 3000\n",
    "# For training experiment (these are good to experiment with if you do not see good results)\n",
    "batch_size = 6 # goes up as increase # images usually\n",
    "learning_rate = 0.0004 # goes down with increased epochs/iterations over data usually\n",
    "\n",
    "######################################## KEEP REST IN CELL THE SAME ########################################\n",
    "config_fname = 'project_files/ssdlite_mobilenet_retrained.config'\n",
    "fine_tune_checkpoint = '\"'+'/home/data/ssdlite_mobilenet_v2_coco_2018_05_09/model.ckpt' + '\"'\n",
    "input_path_train = '\"' + './outputs/objects_train.record-00000-of-00001' + '\"'\n",
    "label_map_path = '\"' + 'objects.pbtxt' + '\"'\n",
    "input_path_eval = '\"' + './outputs/objects_val.record-00000-of-00001' + '\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89a2b24",
   "metadata": {},
   "source": [
    "## Connect to the Azure ML Workspace\n",
    "\n",
    "This step automatically looks for the `config.json` file base directory. You may download your `config.json` from the Azure Portal Azure ML Workspace resource - in the Overview pane.  Then you may drag and drop the `config.json` from your local file system/machine into the file explorer to the left in JupyterLab .\n",
    "\n",
    "The first time you run this cell it will ask you to perform interactive log in to Azure in another browser window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228fc75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14157cbb",
   "metadata": {},
   "source": [
    "## Create project directory (continue on here _after_ you have labeled data)\n",
    "\n",
    "This directory is special in that the contents are uploaded to the Azure ML Compute for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1719c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('project_files'):\n",
    "    os.makedirs('project_files', exist_ok=True)\n",
    "else:\n",
    "    print('project_scripts directory already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d0ff95",
   "metadata": {},
   "source": [
    "Download your COCO formated annotations from the Azure ML Studio labeling project and drag-and-drop them into this new `project_files` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b8c556",
   "metadata": {},
   "source": [
    "## Create and/or reuse a GPU cluster for training\n",
    "\n",
    "Here, the GPU cluster is being made.  You may wish to set `max_nodes` to limit the number of compute nodes that get used.  There is a dedicated VM cluster type and another type that uses low priority nodes for when you are not in a hurry and want to save on resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f65b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for your CPU cluster\n",
    "gpu_cluster_name = \"gpu-cluster\"\n",
    "\n",
    "# Verify that the cluster does not exist already\n",
    "try:\n",
    "    gpu_cluster = ComputeTarget(workspace=ws, name=gpu_cluster_name)\n",
    "    print('Found existing cluster, use it.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6',\n",
    "                                                           idle_seconds_before_scaledown=2400,\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=4)\n",
    "    gpu_cluster = ComputeTarget.create(ws, gpu_cluster_name, compute_config)\n",
    "\n",
    "gpu_cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba0b7d1",
   "metadata": {},
   "source": [
    "## Define an environment\n",
    "\n",
    "Here, instead of using an Azure ML base image, we are creating our own.  In fact, we are also managing all of our own dependencies.  The next cell creates a Python Environment for our training experiment with the TensorFlow Object Detection API and other necessary components like a pretrained model.  View the `TFObjectDetectionAPI.dockerfile` to see more on how this is made.  Note:  the first time this experiment is run, this training docker image is built and it could take up to 30min to complete an experiment due to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb29c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_env = Environment(\"tfobjectdetection\")\n",
    "tf_env.docker.base_image = None\n",
    "tf_env.docker.base_dockerfile = \"./docker/TFObjectDetectionAPI.dockerfile\"\n",
    "tf_env.python.user_managed_dependencies = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27839980",
   "metadata": {},
   "source": [
    "## Labels\n",
    "\n",
    "Add more labels as needed in the format (1-indexed):\n",
    "\n",
    "```\n",
    "item {\n",
    "  id: <number in sequence>\n",
    "  name: 'object_name'\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee9ce55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile project_files/objects.pbtxt\n",
    "item {\n",
    "  id: 1\n",
    "  name: 'mouse'\n",
    "}\n",
    "\n",
    "item {\n",
    "  id: 2\n",
    "  name: 'keyboard'\n",
    "}\n",
    "\n",
    "item {\n",
    "  id: 3\n",
    "  name: 'headphones'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c18cfd",
   "metadata": {},
   "source": [
    "## Scripts to format data and train model\n",
    "\n",
    "**Converting from COCO format annotations and images to TFRecord**\n",
    "\n",
    "This is a utlitiy script that converts COCO format annotations and images to TFRecord format.  We need the annotation file from Azure ML labeling project and the images placed into the defualt Datastore as a Dataset (we should have made this when labeling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5634aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile project_files/tf_record_utils.py\n",
    "\n",
    "import hashlib\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import contextlib2\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "\n",
    "from pycocotools import mask\n",
    "import tensorflow as tf\n",
    "\n",
    "from object_detection.dataset_tools import tf_record_creation_util\n",
    "from object_detection.utils import dataset_util\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "\n",
    "def create_tf_example(image,\n",
    "                      annotations_list,\n",
    "                      image_dir,\n",
    "                      category_index):\n",
    "    \"\"\"Converts image and annotations to a tf.Example proto.\n",
    "    Args:\n",
    "    image: dict with keys:\n",
    "      [u'license', u'file_name', u'coco_url', u'height', u'width',\n",
    "      u'date_captured', u'flickr_url', u'id']\n",
    "    annotations_list:\n",
    "      list of dicts with keys:\n",
    "      [u'segmentation', u'area', u'iscrowd', u'image_id',\n",
    "      u'bbox', u'category_id', u'id']\n",
    "      Notice that bounding box coordinates in the official COCO dataset are\n",
    "      given as [x, y, width, height] tuples using absolute coordinates where\n",
    "      x, y represent the top-left (0-indexed) corner.  This function converts\n",
    "      to the format expected by the Tensorflow Object Detection API (which is\n",
    "      which is [ymin, xmin, ymax, xmax] with coordinates normalized relative\n",
    "      to image size).\n",
    "    image_dir: directory containing the image files.\n",
    "    category_index: a dict containing COCO category information keyed\n",
    "      by the 'id' field of each category.  See the\n",
    "      label_map_util.create_category_index function.\n",
    "    include_masks: Whether to include instance segmentations masks\n",
    "      (PNG encoded) in the result. default: False.\n",
    "    Returns:\n",
    "    example: The converted tf.Example\n",
    "    num_annotations_skipped: Number of (invalid) annotations that were ignored.\n",
    "    Raises:\n",
    "    ValueError: if the image pointed to by data['filename'] is not a valid JPEG\n",
    "    \"\"\"\n",
    "    image_height = 616 # use fixed from data collection notebook for Percept instead of image['height']\n",
    "    image_width = 816 # use fixed from data collection notebook for Percept instead image['width']\n",
    "    filename = image['file_name']\n",
    "    # Remove first part of path because we don't need (name of dataset)\n",
    "    filename = (os.sep).join(filename.split(os.sep)[1:])\n",
    "    image_id = image['id']\n",
    "\n",
    "    full_path = os.path.join(image_dir, filename)\n",
    "    print(full_path)\n",
    "    with tf.gfile.GFile(full_path, 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = PIL.Image.open(encoded_jpg_io)\n",
    "    key = hashlib.sha256(encoded_jpg).hexdigest()\n",
    "\n",
    "    xmin = []\n",
    "    xmax = []\n",
    "    ymin = []\n",
    "    ymax = []\n",
    "    is_crowd = []\n",
    "    category_names = []\n",
    "    category_ids = []\n",
    "    area = []\n",
    "    encoded_mask_png = []\n",
    "    num_annotations_skipped = 0\n",
    "    for object_annotations in annotations_list:\n",
    "        (x, y, width, height) = tuple(object_annotations['bbox'])\n",
    "        x, width = int(x * image_width), int(width * image_width)\n",
    "        y, height = int(y * image_height), int(height * image_height)\n",
    "        if width <= 0 or height <= 0:\n",
    "            num_annotations_skipped += 1\n",
    "            continue\n",
    "        if x + width > image_width or y + height > image_height:\n",
    "            num_annotations_skipped += 1\n",
    "            continue\n",
    "        xmin.append(float(x) / image_width)\n",
    "        xmax.append(float(x + width) / image_width)\n",
    "        ymin.append(float(y) / image_height)\n",
    "        ymax.append(float(y + height) / image_height)\n",
    "        category_id = int(object_annotations['category_id'])\n",
    "        category_ids.append(category_id)\n",
    "        category_names.append(category_index[category_id]['name'].encode('utf8'))\n",
    "        area.append(object_annotations['area'])\n",
    "\n",
    "    feature_dict = {\n",
    "      'image/height':\n",
    "          dataset_util.int64_feature(image_height),\n",
    "      'image/width':\n",
    "          dataset_util.int64_feature(image_width),\n",
    "      'image/filename':\n",
    "          dataset_util.bytes_feature(filename.encode('utf8')),\n",
    "      'image/source_id':\n",
    "          dataset_util.bytes_feature(str(image_id).encode('utf8')),\n",
    "      'image/key/sha256':\n",
    "          dataset_util.bytes_feature(key.encode('utf8')),\n",
    "      'image/encoded':\n",
    "          dataset_util.bytes_feature(encoded_jpg),\n",
    "      'image/format':\n",
    "          dataset_util.bytes_feature('jpeg'.encode('utf8')),\n",
    "      'image/object/bbox/xmin':\n",
    "          dataset_util.float_list_feature(xmin),\n",
    "      'image/object/bbox/xmax':\n",
    "          dataset_util.float_list_feature(xmax),\n",
    "      'image/object/bbox/ymin':\n",
    "          dataset_util.float_list_feature(ymin),\n",
    "      'image/object/bbox/ymax':\n",
    "          dataset_util.float_list_feature(ymax),\n",
    "      'image/object/class/text':\n",
    "          dataset_util.bytes_list_feature(category_names),\n",
    "      'image/object/is_crowd':\n",
    "          dataset_util.int64_list_feature(is_crowd),\n",
    "      'image/object/area':\n",
    "          dataset_util.float_list_feature(area),\n",
    "    }\n",
    "    example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n",
    "    return key, example, num_annotations_skipped\n",
    "\n",
    "\n",
    "def create_tf_record_from_coco_annotations(annotations_file,\n",
    "                                           image_dir,\n",
    "                                           output_path,\n",
    "                                           num_shards=10):\n",
    "    \"\"\"Loads COCO annotation json files and converts to tf.Record format.\n",
    "    Args:\n",
    "    annotations_file: JSON file containing bounding box annotations.\n",
    "    image_dir: Directory containing the image files.\n",
    "    output_path: Path to output tf.Record file.\n",
    "    include_masks: Whether to include instance segmentations masks\n",
    "      (PNG encoded) in the result. default: False.\n",
    "    num_shards: number of output file shards.\n",
    "    \"\"\"\n",
    "    with contextlib2.ExitStack() as tf_record_close_stack, \\\n",
    "            tf.gfile.GFile(annotations_file, 'r') as fid:\n",
    "        output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(\n",
    "            tf_record_close_stack, output_path, num_shards)\n",
    "        groundtruth_data = json.load(fid)\n",
    "        images = groundtruth_data['images']\n",
    "        category_index = label_map_util.create_category_index(\n",
    "            groundtruth_data['categories'])\n",
    "\n",
    "        annotations_index = {}\n",
    "        if 'annotations' in groundtruth_data:\n",
    "            tf.logging.info(\n",
    "                'Found groundtruth annotations. Building annotations index.')\n",
    "            for annotation in groundtruth_data['annotations']:\n",
    "                image_id = annotation['image_id']\n",
    "                if image_id not in annotations_index:\n",
    "                    annotations_index[image_id] = []\n",
    "                annotations_index[image_id].append(annotation)\n",
    "        missing_annotation_count = 0\n",
    "        for image in images:\n",
    "            image_id = image['id']\n",
    "            if image_id not in annotations_index:\n",
    "                missing_annotation_count += 1\n",
    "                annotations_index[image_id] = []\n",
    "        tf.logging.info('%d images are missing annotations.',\n",
    "                        missing_annotation_count)\n",
    "\n",
    "        total_num_annotations_skipped = 0\n",
    "        for idx, image in enumerate(images):\n",
    "            if idx % 100 == 0:\n",
    "                tf.logging.info('On image %d of %d', idx, len(images))\n",
    "            annotations_list = annotations_index[image['id']]\n",
    "            _, tf_example, num_annotations_skipped = create_tf_example(\n",
    "                image, annotations_list, image_dir, category_index)\n",
    "            total_num_annotations_skipped += num_annotations_skipped\n",
    "            shard_idx = idx % num_shards\n",
    "            output_tfrecords[shard_idx].write(tf_example.SerializeToString())\n",
    "        tf.logging.info('Finished writing, skipped %d annotations.',\n",
    "                    total_num_annotations_skipped)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebe4888",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile project_files/__init__.py\n",
    "\"\"\"\n",
    "Created to be able to import the util function in our train.py script\n",
    "\"\"\"\n",
    "from .tf_record_utils import create_tf_record_from_coco_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d645c3b",
   "metadata": {},
   "source": [
    "Prepare model config variables, hyperparameters and file paths for a tensorflow configuration file for SSDLite-Mobilenetv2 model training.\n",
    "\n",
    "IMPORTANT:  update `num_classes`, `batch_size` and `learning_rate` as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a67b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $config_fname\n",
    "model {\n",
    "  ssd {\n",
    "    num_classes: $num_classes\n",
    "    box_coder {\n",
    "      faster_rcnn_box_coder {\n",
    "        y_scale: 10.0\n",
    "        x_scale: 10.0\n",
    "        height_scale: 5.0\n",
    "        width_scale: 5.0\n",
    "      }\n",
    "    }\n",
    "    matcher {\n",
    "      argmax_matcher {\n",
    "        matched_threshold: 0.5\n",
    "        unmatched_threshold: 0.5\n",
    "        ignore_thresholds: false\n",
    "        negatives_lower_than_unmatched: true\n",
    "        force_match_for_each_row: true\n",
    "      }\n",
    "    }\n",
    "    similarity_calculator {\n",
    "      iou_similarity {\n",
    "      }\n",
    "    }\n",
    "    anchor_generator {\n",
    "      ssd_anchor_generator {\n",
    "        num_layers: 6\n",
    "        min_scale: 0.2\n",
    "        max_scale: 0.95\n",
    "        aspect_ratios: 1.0\n",
    "        aspect_ratios: 2.0\n",
    "        aspect_ratios: 0.5\n",
    "        aspect_ratios: 3.0\n",
    "        aspect_ratios: 0.3333\n",
    "      }\n",
    "    }\n",
    "    image_resizer {\n",
    "      fixed_shape_resizer {\n",
    "        height: 300\n",
    "        width: 300\n",
    "      }\n",
    "    }\n",
    "    box_predictor {\n",
    "      convolutional_box_predictor {\n",
    "        min_depth: 0\n",
    "        max_depth: 0\n",
    "        num_layers_before_predictor: 0\n",
    "        use_dropout: false\n",
    "        dropout_keep_probability: 0.8\n",
    "        kernel_size: 3\n",
    "        use_depthwise: true\n",
    "        box_code_size: 4\n",
    "        apply_sigmoid_to_scores: false\n",
    "        conv_hyperparams {\n",
    "          activation: RELU_6,\n",
    "          regularizer {\n",
    "            l2_regularizer {\n",
    "              weight: 0.00004\n",
    "            }\n",
    "          }\n",
    "          initializer {\n",
    "            truncated_normal_initializer {\n",
    "              stddev: 0.03\n",
    "              mean: 0.0\n",
    "            }\n",
    "          }\n",
    "          batch_norm {\n",
    "            train: true,\n",
    "            scale: true,\n",
    "            center: true,\n",
    "            decay: 0.9997,\n",
    "            epsilon: 0.001,\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    feature_extractor {\n",
    "      type: 'ssd_mobilenet_v2'\n",
    "      min_depth: 16\n",
    "      depth_multiplier: 1.0\n",
    "      use_depthwise: true\n",
    "      conv_hyperparams {\n",
    "        activation: RELU_6,\n",
    "        regularizer {\n",
    "          l2_regularizer {\n",
    "            weight: 0.00004\n",
    "          }\n",
    "        }\n",
    "        initializer {\n",
    "          truncated_normal_initializer {\n",
    "            stddev: 0.03\n",
    "            mean: 0.0\n",
    "          }\n",
    "        }\n",
    "        batch_norm {\n",
    "          train: true,\n",
    "          scale: true,\n",
    "          center: true,\n",
    "          decay: 0.9997,\n",
    "          epsilon: 0.001,\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    loss {\n",
    "      classification_loss {\n",
    "        weighted_sigmoid {\n",
    "        }\n",
    "      }\n",
    "      localization_loss {\n",
    "        weighted_smooth_l1 {\n",
    "        }\n",
    "      }\n",
    "      hard_example_miner {\n",
    "        num_hard_examples: 3000\n",
    "        iou_threshold: 0.99\n",
    "        loss_type: CLASSIFICATION\n",
    "        max_negatives_per_positive: 3\n",
    "        min_negatives_per_image: 3\n",
    "      }\n",
    "      classification_weight: 1.0\n",
    "      localization_weight: 1.0\n",
    "    }\n",
    "    normalize_loss_by_num_matches: true\n",
    "    post_processing {\n",
    "      batch_non_max_suppression {\n",
    "        score_threshold: 1e-8\n",
    "        iou_threshold: 0.6\n",
    "        max_detections_per_class: 100\n",
    "        max_total_detections: 100\n",
    "      }\n",
    "      score_converter: SIGMOID\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "train_config: {\n",
    "  batch_size: $batch_size\n",
    "  optimizer {\n",
    "    rms_prop_optimizer: {\n",
    "      learning_rate: {\n",
    "        exponential_decay_learning_rate {\n",
    "          initial_learning_rate: $learning_rate\n",
    "          decay_steps: $decay_steps\n",
    "          decay_factor: 0.95\n",
    "        }\n",
    "      }\n",
    "      momentum_optimizer_value: 0.9\n",
    "      decay: 0.9\n",
    "      epsilon: 1.0\n",
    "    }\n",
    "  }\n",
    "  fine_tune_checkpoint: $fine_tune_checkpoint\n",
    "  fine_tune_checkpoint_type:  \"detection\"\n",
    "  num_steps: $num_epochs\n",
    "  data_augmentation_options {\n",
    "    random_horizontal_flip {\n",
    "    }\n",
    "  }\n",
    "  data_augmentation_options {\n",
    "    ssd_random_crop {\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "train_input_reader: {\n",
    "  tf_record_input_reader {\n",
    "    input_path: $input_path_train\n",
    "  }\n",
    "  label_map_path: $label_map_path\n",
    "}\n",
    "\n",
    "eval_config: {\n",
    "  num_examples: 100\n",
    "  # Note: The below line limits the evaluation process to 10 evaluations.\n",
    "  # Remove the below line to evaluate indefinitely.\n",
    "  max_evals: 10\n",
    "}\n",
    "\n",
    "eval_input_reader: {\n",
    "  tf_record_input_reader {\n",
    "    input_path: $input_path_eval\n",
    "  }\n",
    "  label_map_path: $label_map_path\n",
    "  shuffle: false\n",
    "  num_readers: 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa6c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update config file paths\n",
    "with open(config_fname, 'r') as f:\n",
    "    config_file = f.read()\n",
    "\n",
    "config_file = config_file.replace('$fine_tune_checkpoint', fine_tune_checkpoint)\n",
    "config_file = config_file.replace('$label_map_path', label_map_path)\n",
    "config_file = config_file.replace('$input_path_train', input_path_train)\n",
    "config_file = config_file.replace('$input_path_eval', input_path_eval)\n",
    "config_file = config_file.replace('$num_classes', str(num_classes))\n",
    "config_file = config_file.replace('$batch_size', str(batch_size))\n",
    "config_file = config_file.replace('$learning_rate', str(learning_rate))\n",
    "config_file = config_file.replace('$decay_steps', str(int(0.8*num_epochs)))\n",
    "config_file = config_file.replace('$num_epochs', str(num_epochs))\n",
    "\n",
    "update_file = open(config_fname, 'w')\n",
    "update_file.writelines(config_file)\n",
    "update_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77cb570",
   "metadata": {},
   "source": [
    "**Train script**\n",
    "\n",
    "Write the training script, `train.py`, to the project folder, `project_files` so that it is uploaded to the Azure ML Compute when the run is started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074fbd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile project_files/train.py\n",
    "\"\"\"\n",
    "Azure ML training script for TF object detection experiment\n",
    "\"\"\"\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "import shutil\n",
    "import glob\n",
    "import urllib\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from uuid import uuid4\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from azureml.core import Datastore\n",
    "from azureml.core.run import Run\n",
    "from azureml.data.data_reference import DataReference\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "from tf_record_utils import create_tf_record_from_coco_annotations\n",
    "\n",
    "\n",
    "# Test greeting\n",
    "def greeting():\n",
    "    \"\"\"Run a test greeting and check on GPU availability\"\"\"\n",
    "    print(\"Welcome to TF OD container!\")\n",
    "    print(tf.__version__)\n",
    "    print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "greeting()\n",
    "\n",
    "def get_args():\n",
    "    \"\"\"Parse arguments to script\"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--ds-path', type=str,\n",
    "                        dest='datastore_path', help='The path to the data on the default datastore')\n",
    "    parser.add_argument('--annot-file', type=str,\n",
    "                        dest='annot_file', help='The COCO format annotation file from Azure ML labeling')\n",
    "    parser.add_argument('--num-epochs', type=int, default=1000,\n",
    "                        dest='num_epochs', help='Number of epochs to train')\n",
    "    parser.add_argument('--num-classes', type=int, default=1,\n",
    "                        dest='num_classes', help='Number of classes')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "def get_data(datastore_path):\n",
    "    \"\"\"Get the data from the default datastore of the workspace\n",
    "    and return the data folder where it is located on compute\"\"\"\n",
    "    data_folder = os.sep + \\\n",
    "                  os.sep.join(os.getcwd().split(os.sep)[:-2]) + \\\n",
    "                  os.sep + datastore_path\n",
    "    return data_folder\n",
    "\n",
    "def prep_data(annot_file, data_folder):\n",
    "    \"\"\"Convert coco data format to TFRecord (with image files in the\n",
    "    default datastore)\"\"\"\n",
    "    # Look at data folder\n",
    "    print('Data folder is at:', data_folder)\n",
    "    print('List all files/folders in data folder: ', os.listdir(data_folder))\n",
    "    \n",
    "    # What is our current working directory?\n",
    "    print(\"Current working directory: {}\".format(os.getcwd()))\n",
    "    print(\"Contents of directory: \")\n",
    "    os.system(\"ls\")\n",
    "    print(\"Contents of /home: \")\n",
    "    os.system(\"ls /home\")\n",
    "    \n",
    "    # Outputs folder - visible in AzureML Studio\n",
    "    os.makedirs('./outputs', exist_ok=True)\n",
    "\n",
    "    train_output_path = os.path.join('./outputs', 'objects_train.record')\n",
    "    val_output_path = os.path.join('./outputs', 'objects_val.record')\n",
    "\n",
    "    # Create the TFRecords from the COCO annotations\n",
    "    create_tf_record_from_coco_annotations(\n",
    "      annot_file,\n",
    "      data_folder,\n",
    "      train_output_path,\n",
    "      num_shards=1)\n",
    "    create_tf_record_from_coco_annotations(\n",
    "      annot_file,\n",
    "      data_folder,\n",
    "      val_output_path,\n",
    "      num_shards=1)\n",
    "\n",
    "def decompress_model(model_file):\n",
    "    \"\"\"Decompress archived model located in the /home/data folder on compute\"\"\"\n",
    "    tar = tarfile.open(model_file)\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "    shutil.move('./ssdlite_mobilenet_v2_coco_2018_05_09',\n",
    "                '/home/data/')\n",
    "\n",
    "def move_config(config_fname):\n",
    "    \"\"\"Move the config file to the TF models repo\"\"\"\n",
    "    shutil.copyfile('ssdlite_mobilenet_retrained.config',\n",
    "                    config_fname)\n",
    "\n",
    "    \n",
    "def train(config_fname, num_epochs):\n",
    "    \"\"\"Run the os command to train the model using transfer learning\n",
    "    and an SSDLite-MobilenetV2 base model\"\"\"\n",
    "    \n",
    "    \n",
    "    cmd = \"PYTHONPATH=$PYTHONPATH:/home/models/research:/home/models/research/slim \\\n",
    "          python /home/models/research/object_detection/model_main.py \\\n",
    "          --pipeline_config_path={} \\\n",
    "          --model_dir={} \\\n",
    "          --alsologtostderr \\\n",
    "          --num_train_steps={} \\\n",
    "          --num_eval_steps={}\".format(config_fname, './outputs', num_epochs, num_epochs)\n",
    "    \n",
    "    os.system(cmd)\n",
    "\n",
    "def export_frozen_graph(config_fname, model_path_prefix):\n",
    "    \"\"\"Export frozen graph from checkpoint\"\"\"\n",
    "    \n",
    "    cmd = \"PYTHONPATH=$PYTHONPATH:/home/models/research:/home/models/research/slim \\\n",
    "            python /home/models/research/object_detection/export_inference_graph.py \\\n",
    "            --input_type=image_tensor \\\n",
    "            --pipeline_config_path={} \\\n",
    "            --output_directory=./outputs/ \\\n",
    "            --trained_checkpoint_prefix={}\".format(config_fname, model_path_prefix)\n",
    "    os.system(cmd)\n",
    "    \n",
    "def test_single_image(data_folder, num_classes):\n",
    "    \"\"\"Run inference with frozen graph on a single image from train dataset\n",
    "    for sanity check\"\"\"\n",
    "    # Path to the frozen graph:\n",
    "    path_to_frozen_graph = './outputs/frozen_inference_graph.pb'\n",
    "\n",
    "    # Path to the label map\n",
    "    path_to_label_map = './objects.pbtxt'\n",
    "\n",
    "    # Test image (first image in list)\n",
    "    image_path = glob.glob(os.path.join(data_folder,'**','*.jpg'), \n",
    "                                         recursive=True)[0]\n",
    "\n",
    "    # Minimum confidence value needed to display the bounding box on the image. In range [0.0, 1.0].\n",
    "    min_threshold = 0.7\n",
    "\n",
    "    # Read the frozen graph\n",
    "    detection_graph = tf.Graph()\n",
    "    with detection_graph.as_default():\n",
    "        od_graph_def = tf.GraphDef()\n",
    "        with tf.gfile.GFile(path_to_frozen_graph, 'rb') as fid:\n",
    "            serialized_graph = fid.read()\n",
    "            od_graph_def.ParseFromString(serialized_graph)\n",
    "            tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "    label_map = label_map_util.load_labelmap(path_to_label_map)\n",
    "    categories = label_map_util.convert_label_map_to_categories(label_map,\n",
    "                                                                max_num_classes=num_classes,\n",
    "                                                                use_display_name=True)\n",
    "    category_index = label_map_util.create_category_index(categories)\n",
    "\n",
    "    # Detection\n",
    "    with detection_graph.as_default():\n",
    "        with tf.Session(graph=detection_graph) as sess:\n",
    "            # Read in the image\n",
    "            image_np = plt.imread(image_path)\n",
    "\n",
    "            # Expand dimensions since the model expects images to have shape: [1, None, None, 3] \n",
    "            image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "\n",
    "            # Extract image tensor\n",
    "            image_tensor = detection_graph.get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "            # Extract detection boxes\n",
    "            boxes = detection_graph.get_tensor_by_name('detection_boxes:0')\n",
    "\n",
    "            # Extract detection scores\n",
    "            scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "\n",
    "            # Extract detection classes\n",
    "            classes = detection_graph.get_tensor_by_name('detection_classes:0')\n",
    "\n",
    "            # Extract number of detections\n",
    "            num_detections = detection_graph.get_tensor_by_name('num_detections:0')\n",
    "\n",
    "            # Actual detection.\n",
    "            boxes, scores, classes, num_detections = sess.run([boxes, scores, classes, num_detections], feed_dict={image_tensor: image_np_expanded})\n",
    "            print(f\"BOXES (shaped {boxes.shape}):\\n{boxes}\")\n",
    "            print(f\"SCORES (shaped {scores.shape}):\\n{scores}\")\n",
    "            print(f\"CLASSES (shaped {classes.shape}):\\n{classes}\")\n",
    "            print(f\"NDETECTIONS (shaped {num_detections.shape}):\\n{num_detections}\")\n",
    "\n",
    "            # Visualization of the results of a detection.\n",
    "            image_np_copy = image_np.copy()\n",
    "            vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "                image_np_copy,\n",
    "                np.squeeze(boxes),\n",
    "                np.squeeze(classes).astype(np.int32),\n",
    "                np.squeeze(scores),\n",
    "                category_index,\n",
    "                use_normalized_coordinates=True,\n",
    "                line_thickness=3,\n",
    "                min_score_thresh=min_threshold\n",
    "                )\n",
    "            plt.imsave('./outputs/test_detections.jpg', image_np_copy)\n",
    "    \n",
    "\n",
    "# Steps:\n",
    "args = get_args()\n",
    "\n",
    "config_fname = '/home/models/research/object_detection/samples/configs/ssdlite_mobilenet_retrained.config'\n",
    "model_path_prefix = './outputs/model.ckpt-{}'.format(args.num_epochs)\n",
    "\n",
    "# Download data\n",
    "data_folder = get_data(args.datastore_path)\n",
    "# Convert annotations and data to TFRecord format\n",
    "prep_data(args.annot_file, data_folder)\n",
    "# Decompress model from archive file\n",
    "decompress_model(os.sep + os.path.join('home', 'data', \n",
    "                                       'ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz'))\n",
    "move_config(config_fname)\n",
    "# Train\n",
    "train(config_fname, args.num_epochs)\n",
    "# Convert TF checkpoint to a TF frozen graph for inferencing\n",
    "export_frozen_graph(config_fname, model_path_prefix)\n",
    "# Test on a single image from our data for sanity check\n",
    "test_single_image(data_folder, num_classes=args.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c245e970",
   "metadata": {},
   "source": [
    "## Run experiment\n",
    "\n",
    "The `ScriptRunConfig` starts the training experiment in the Azure ML Workspace on our GPU cluster defined above as our `compute_target`.  The training script will:\n",
    "- Convert the COCO annotations and images to TFRecord as our input data\n",
    "- Train our model with the TensorFlow Object Detection API we built for our compute\n",
    "- Convert the model checkpoint output into a frozen graph and place it in the `outputs` folder in the Workspace\n",
    "\n",
    "In the following cell replace `<add-your-coco-annot-filename-here.json>` with the name of the COCO annotation file that you placed in your `project_files` folder.\n",
    "\n",
    "IMPORTANT:  this cell will run until Azure ML has completed training the model (it is a blocking method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5278429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training script args - take a look at training script to see how they are used\n",
    "script_args = ['--ds-path', 'office_supplies', # may wish to change this name\n",
    "               '--annot-file', '<add-your-coco-annot-filename-here.json>',\n",
    "               '--num-epochs', num_epochs,\n",
    "               '--num-classes', num_classes]\n",
    "\n",
    "tf_config = ScriptRunConfig(source_directory='project_files',\n",
    "                                script='train.py',\n",
    "                                arguments=script_args,\n",
    "                                compute_target=gpu_cluster,\n",
    "                                environment=tf_env)\n",
    "\n",
    "# Create experiment object and give experiment a name - change name as needed\n",
    "exp = Experiment(ws, 'tf-od-custom-office-supplies')\n",
    "\n",
    "# Submit experiment.  This is blocking - comment out \"wait_for_completion\" if you do not wish for this\n",
    "run_dict = exp.submit(tf_config).wait_for_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab731c7",
   "metadata": {},
   "source": [
    "Now, go to the Azure Portal and Azure ML Studio to check on the progress of the Run.  You may move on in the notebook once the Run has finished.\n",
    "\n",
    "https://ml.azure.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078d15db",
   "metadata": {},
   "source": [
    "## Register and download model\n",
    "\n",
    "Run the following cell **when the run above has completed** in the Azure Portal.  The run will not be able to register the model files, otherwise.\n",
    "\n",
    "Here, we want to register the model based on the experiment and run above, so that we can download the files.  We register the entire `outputs` directory from Azure ML Compute so that we can access multiple files as our model checkpoint consists of several files (registering an entire directory is an option when you register models with Azure ML).\n",
    "\n",
    "We also download the model files and anything in the `outputs` folder from the compute to the `experiment_outputs` directory, a new directory for the outputs of the Azure ML experiment locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb9970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the correct run based on what was returned in experiment above\n",
    "# If you do not have the \"run_dict\" object, then search directly by run id\n",
    "# (which you can find in the Azure ML Studio from the Portal)\n",
    "print(run_dict.id)\n",
    "\n",
    "run_list = exp.get_runs()\n",
    "run = None\n",
    "for run_in_list in run_list:\n",
    "    if run_in_list.id == run_dict.id:\n",
    "#     if run_in_list.id == 'a specific run id':\n",
    "        run = run_in_list\n",
    "    \n",
    "print(run.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa028ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register model files from the outputs folder - tags can be any dictionary\n",
    "model = run.register_model(model_name='tf_object_detection_api_custom',\n",
    "                           tags={'task': 'object detection',\n",
    "                                 'dataset': 'Office supplies',\n",
    "                                 'framework': 'TensorFlow'},\n",
    "                           model_path='outputs/frozen_inference_graph.pb')\n",
    "print(model.name, model.id, model.version, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8833296e",
   "metadata": {},
   "source": [
    "Now, let's download the model files locally (may take some time).  They will appear in a folder called `experiment_outputs` along with the TFRecord dataset files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e638e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model\n",
    "if not os.path.exists('experiment_outputs'):\n",
    "    os.makedirs('experiment_outputs', exist_ok=True)\n",
    "model.download(target_dir='./experiment_outputs', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e68bf",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- Convert the model to OpenVINO format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7670e1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
